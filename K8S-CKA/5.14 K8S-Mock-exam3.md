### Mock Exams

# Mock Exam 3

# 1. Task
Create a new service account with the name `pvviewer`. Grant this Service account access to list all `PersistentVolumes` in the cluster by creating an appropriate cluster role called `pvviewer-role` and ClusterRoleBinding called `pvviewer-role-binding`.
Next, create a pod called `pvviewer` with the image: `redis` and serviceAccount: `pvviewer` in the default namespace.
# 1. Solution
kubectl create serviceaccount pvviewer
kubectl create clusterrole pvviewer-role --resource=persistentvolumes --verb=list
kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer
kubectl run pvviewer --image=redis --dry-run=client -o yaml > pvviewer.yaml
edit manifest, add SA:
containers:
...
serviceAccountName: pvviewer

kubectl apply -f pvviewer


# 2. Task 
List the `InternalIP` of all nodes of the cluster. Save the result to a file `/root/CKA/node_ips.`
Answer should be in the format: 
InternalIP of controlplane  InternalIP of node01 (in a single line)
# 2. Solution
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips


# 3. Task
kubectl run multi-pod --image=nginx --dry-run=client -o yaml > multi-pod.yaml
# 3. Solution
kubectl run multi-pod --image=busybox --comand sleep 4800 --dry-run=client -o yaml > multi-pod.yaml
edit manifest file with this contents:
containers:
- image: nginx
  name: alpha
  env:
  - name: name
    value: alpha
- image: busybox
  name: beta
  command: ["sleep", "4800"]
  env:
  - name: name
    value: beta
kubectl apply -f multi-pod.yaml


# 4. Task
Create a Pod called `non-root-pod` image: `redis:alpine`
`runAsUser: 1000`
`fsGroup: 2000`
# 4. Solution
kubectl run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root.yaml
edit manifest file, add contens:
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
...
kubectl apply -f non-root.yaml


# 5. Task
We have deployed a new pod called `np-test-1` and a service called `np-test-service`. `Incoming` connections to this service are not working. Troubleshoot and fix it.
Create `NetworkPolicy`, by the name `ingress-to-nptest` that allows `ingress` connections to the service over port `80`.
# 5. Solution
Use this template or take template from docs

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
  - Ingress
  ingress:
  - ports:
    - protocol: TCP
      port: 80


# 6. Task
Taint the worker node `node01` to be Unschedulable. Once done, create a pod called `dev-redis`, image `redis:alpine`, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called `prod-redis` and image: `redis:alpine` with toleration to be scheduled on `node01`.
key: `env_type`, value: `production`, operator: `Equal` and effect: `NoSchedule`
# 6. Solution
kubectl taint node node01 env_type=production:NoSchedule
kubectl run dev-redis --image=redis:alpine
kubectl get pods -o wide

cat <<EOF >> prod-redis.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
spec:
  containers:
  - name: prod-redis
    image: redis:alpine
  tolerations:
  - effect: NoSchedule
    key: env_type
    operator: Equal
    value: production
EOF
kubectl apply -f prod-redis.yaml
kubectl get pods -owide | grep prod-redis

# 7. Task
Create a pod called `hr-pod` in `hr` namespace labeled `production=environment` and `frontend=tier` image: `redis:alpine`
# 7. Solution
kubectl -n hr run hr-pod --image=redis:alpine --labels "production=environment,frontend=tier" --dry-run=client -o yaml > hr-pod.yaml

# 8. Task
A kubeconfig file called `super.kubeconfig` has been created under `/root/CKA`. There is something wrong with the configuration. Troubleshoot and fix it.
# 8. Solution
vi /root/CKA/super.kubeconfig
fix controlplane port by default 6443
check status of cluster
kubectl cluster-info --kubeconfig=/root/CKA/super.kubeconfig

# 9. Task
We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.
# 9. Solution
kubectl scale deployment nginx-deploy --replicas=3
kubectl get deploy 
kubectl get all -A 
check pods in kube-system namespace, notice wrong name of kube-controller-manager pod.
ls /etc/kubernetes/manifests
cd /etc/kubernetes/manifests
cat kube-controller-manager.yaml
sed -i 's/kube-contro1ler-manager/kube-controller-manager/g' kube-controller-manager.yaml
kubectl get pods -n kube-system

