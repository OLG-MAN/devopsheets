### Mock Exams

# Mock Exam 2

# 1. Task
Take a backup of the etcd cluster and save it to /opt/etcd-backup.db
# 1. Solution
ETCDCTL_API=3 etcdctl snapshot save /opt/etcd-backup.db \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key


# 2. Task 
Create a Pod called `redis-storage` with image `redis:alpine` with a Volume of type `emptyDir` that lasts for the life of the Pod.
Specs on the below.
Pod named `redis-storage` created
Pod `redis-storage` uses Volume type of `emptyDir`
Pod `redis-storage` uses volumeMount with `mountPath = /data/redis`
# 2. Solution
kubectl run redis-storage --image=redis:alpine --dry-run=client -o yaml > redis-storage.yaml
add to file:
volumes:
 - name: redis-storage
   emptyDir: {}
...
containers:
...
volumeMounts:
   - name: redis-storage
     mountPath: /data/redis
kubectl apply -f redis-storage.yaml


# 3. Task
Create a new pod called `super-user-pod` with image `busybox:1.28`. Allow the pod to be able to set `system_time`.
The container should `sleep` for `4800 seconds`.
# 3. Solution
kubectl run super-user-pod --image=busybox:1.28 --dry-run=client -o yaml > super-user-pod.yaml
add to manifest file:
containers:
...
  command: ["sleep", "4800"]
    securityContext:
      capabilities:
        add: ["SYS_TIME"]


# 4. Task
A pod definition file is created at `/root/CKA/use-pv.yaml`. Make use of this manifest file and mount the persistent volume called `pv-1`. Ensure the pod is running and the PV is bound. mountPath: `/data`  persistentVolumeClaim Name: `my-pvc`
# 4. Solution
create pvc.yaml file with this content:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:  
    requests:
      storage: 10Mi 

edit file /root/CKA/use-pv.yaml add this:
containers:
...
  volumeMounts:
    - mountPath: "/data"
      name: mypod
volumes:
- name: mypod
  persistentVolumeClaim:
    claimName: my-pvc 

kubectl apply -f pvc.yaml
kubectl apply -f /root/CKA/use-pv.yaml


# 5. Task
Create a new deployment called `nginx-deploy`, with image `nginx:1.16` and `1`(!) replica. Next upgrade the deployment to version `1.17` using rolling update. Record the changes for the image upgrade
# 5. Solution
kubectl create deployment nginx-deploy --image=nginx:1.16 --dry-run=client -o yaml > deploy.yaml
kubectl create -f deploy.yaml --record
kubectl rollout history deploy nginx-deploy
kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record
kubectl rollout history deploy nginx-deploy


# 6. Task
Create a new user called `john`. Grant him access to the cluster. John should have permission to `create`, `list`, `get`, `update` and `delete` pods in the `development` namespace. 
The private key exists in the location: `/root/CKA/john.key` and csr at `/root/CKA/john.csr`
Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.
# 6. Solution
Get template of CSR manifest in k8s docs or use manifest below:

---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  signerName: kubernetes.io/kube-apiserver-client
  request: <key>
  usages:
  - digital signature
  - key encipherment
  - client auth

Edit manifest file, substitute name, expiration value and key with /root/CKA/john.csr file
cat /root/CKA/john.csr | base64 | tr -d "\n"
kubectl apply -f john-developer.yaml 
k get csr
kubectl certificate approve john-developer
kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development
kubectl create rolebinding developer-role-binding --role=developer --user=john --namespace=development
kubectl auth can-i update pods --as=john --namespace=development


# 7. Task
Create a nginx pod called `nginx-resolver` using image `nginx`, expose it internally with a service called `nginx-resolver-service`.
Test that you are able to look up the service and pod names from within the cluster. Use the image `busybox:1.28` to create a pod for dns lookup. Record results in `/root/CKA/nginx.svc` and `/root/CKA/nginx.pod` for `service` and `pod` name resolutions respectively
# 7. Solution
kubectl run nginx-resolver --image=nginx
kubectl expose pod nginx-resolver --name=nginx-resolver-service --port=80 --target-port=80 --type=ClusterIP
kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service
kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service > /root/CKA/nginx.svc
# Get the IP of the nginx-resolver pod and replace the dots(.) with hyphon(-) which will be used below.
kubectl get pod nginx-resolver -o wide
kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup <P-O-D-I-P.default.pod> > /root/CKA/nginx.pod


# 8. Task
Create a static pod on `node01` called `nginx-critical` with image `nginx` and make sure that it is recreated/restarted automatically in case of a failure.
Use `/etc/kubernetes/manifests` as the Static Pod path for example.
# 8. Solution
kubectl run nginx-critical --image=nginx --dry-run=client -o yaml > static.yaml
cat static.yaml - Copy the contents of this file.
ssh node01 
cd /etc/kubernetes/manifests
vi nginx-critical.yaml 
paste copied data and save manifest file
Go back to master node
kubectl get pods 


