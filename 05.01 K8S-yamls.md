### Yaml Examples

### Simple Template manifest
```
apiVersion: <v1>/<v1>/<apps/v1>/<apps/v1>
kind: <Pod>/<Service>/<ReplicaSet>/<Deployment>
metadata:
  name: <myapp-pod>
  labels:
    app: <myapp/name>
    type: <frontend/backend/db/etc>
spec:
  containers:
  - name: <nginx-container-name>
    image: <nginx:latest>
```

### Simple ReplicaSet manifest
```
apiVersion: <apps/v1>
kind: <ReplicaSet>
metadata:
  name: <myapp-replicaset>
  labels:
    app: <myapp>
    type: <frontend>
spec:
  template:
    metadata:
        name: <myapp-pod>
        labels:
          app: <myapp>
          type: <frontend>
    spec:
      containers:
      - name: <nginx-container-name>
        image: <nginx:latest>
  replicas: <3>
  selector:
    matchLabels:
      type: <frontend>
```

### Simple Deployment manifest
apiVersion: <apps/v1>
kind: <Deployment>
metadata:
  name: <myapp-deplyment>
  labels:
    app: <myapp>
    type: <frontend>
spec:
  replicas: <3>
  selector:
    matchLabels:
      type: <frontend>
  strategy:
    rollingUpdate:
      maxSurge: <25%>
      maxUnavailable: <25%>
    type: <RollingUpdate>
  template:
    metadata:
      name: <myapp-pod>
      labels:
        app: <myapp>
        type: <frontend>
    spec:
      containers:
      - name: <nginx-container-name>
        image: <nginx:latest>


### Simple Service manifest
apiVersion: <v1>
kind: <Service>
  name: <myapp-pod>
spec:
  type: <NodePort>
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  selector:
    app: <app-label-name>
    type: <frontend-label-of-pod>


### Simple Namespace manifest
apiVersion: <v1>
kind: <Namespace>
  name: <ns-name>


### Namespace manifest with quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi


### Pod binding examle manifest
apiVerion: <v1>
kind: <Binding>
metadata:
  name: <pod-name>
target:
  apiVersion: v1
  kind: Node
  name: <target-node-name> 
  

### DaemonSet example manifest
apiVersion: <apps/v1>
kind: <DaemonSet>
metadata:
  labels:
    app: <elasticsearch>
  name: <elasticsearch>
  namespace: <kube-system>
spec:
  selector:
    matchLabels:
      app: <elasticsearch>
  template:
    metadata:
      labels:
        app: <elasticsearch>
    spec:
      containers:
      - image: <k8s.gcr.io/fluentd-elasticsearch:1.20>
        name: <fluentd-elasticsearch>


### Additional Scheduler
## config .yaml
apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false
## apply configmap
kubectl create -n kube-system configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml


## my-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: k8s.gcr.io/kube-scheduler:v1.23.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config


### CertificateSigningRequest
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
  - system:authenticated
  request: 
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth


### Kubeconfig file. Example. Single cluster/user.
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: <certificate-data>
    server: https://controlplane:6443
  name: <cluster-name>
contexts:
- context:
    cluster: <cluster-name>
    user: <cluster-user-name>
  name: <cluster-user-name>@<cluster-name>
current-context: <cluster-user-name>@<cluster-name>
kind: Config
preferences: {}
users:
- name: <cluster-user-name>
  user:
    client-certificate-data: <certificate-data>
    key-data: <key-data>


### KubeConfig file. Multi cluesters/users. Example.
apiVersion: v1
kind: Config
clusters:
- name: <production>
  cluster:
    certificate-authority: </etc/kubernetes/pki/ca.crt>
    server: <https://controlplane:6443>
- name: <development>
  cluster:
    certificate-authority: </etc/kubernetes/pki/ca.crt>
    server: <https://controlplane:6443>
- name: <kubernetes-on-aws>
  cluster:
    certificate-authority: </etc/kubernetes/pki/ca.crt>
    server: <https://controlplane:6443>

contexts:
- name: <test-user@development>
  context:
    cluster: <development>
    user: <test-user>
- name: <aws-user@kubernetes-on-aws>
  context:
    cluster: <kubernetes-on-aws>
    user: <aws-user>
- name: <test-user@production>
  context:
    cluster: <production>
    user: <test-user>
    namespace: <taget-ns-name>

users:
- name: <test-user>
  user:
    client-certificate: </etc/kubernetes/pki/users/test-user/test-user.crt>
    client-key: </etc/kubernetes/pki/users/test-user/test-user.key>
- name: <aws-user>
  user:
    client-certificate: </etc/kubernetes/pki/users/aws-user/aws-user.crt>
    client-key: </etc/kubernetes/pki/users/aws-user/aws-user.key>

current-context: <test-user@development>
preferences: {}


### ClusterRole. Example with permisisons to pod and cm
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]                     # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "list", "update", "create"]
- apiGroups: [""]
  resources: ["ConfigMap"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["delete"]
  resourceNames: ["pod1", "pod2"]     # perms to delete Only target pods


### RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io


### Cluster Role
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: <cluster-administrator>
rules:
- apiGroups: [""]s
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]


### Cluster Role Binding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: <cluster-admin-role-binding>
subjects:
- kind: User
  name: <cluster-admin>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: <cluster-administrator>
  apiGroup: rbac.authorization.k8s.io


### Pod with using private docker registry image
apiVersion: <v1>
kind: <Pod>
metadata:
  name: <nginx-pod>
spec:
  containers:
  - name: <nginx>
    image: <private-registry.io/apps/internal-app>
  imagePullSecrets:
  - name: <regcred>


### Network Policies. Example.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
 name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db                       # target pod for policy
  policyTypes:
  - Ingress
  - Egress                          
  ingress:                      
  - from:
    - podSelector:                   # target ingress by labels
        matchLabels:
          role: api-pod
    ports:                           # target port
    - protocol: TCP
      port: 3306
    - ipBlock:
        cidr: 192.168.5.10/32        # target ingress by ip/range
  egress:
  - to:
    - ipBlock:
        cidr: 192.168.5.10/32        # target egress by ip
    ports:
    - protocol: TCP                  # egress protocol to target vm 
    port: 80
### Networking Policies. Example 2 egress policies.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP


### Persistance Volume Claim Example
apiVersion: <v1>
kind: <PersistentVolume>
metadata:
  name: <pv-vol1>
spec:
  accessModes:
  - ReadWriteOnce              # ReadOnlyMany/ReadWriteOnce/ReadWriteMany
  capacity:
   storage: 1Gi
  hostPath:
   path: /tmp/data


### Persistance Volume Claim Example
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: <myclaim>
spec:
  accessModes: 
  - ReadWriteOnce
  resources:
   requests:
     storage: 1Gi


### Pod with PVC Example (Use after PVC created)
apiVersion: v1
kind: Pod
metadata:
  name: <mypod>
spec:
  containers:
    - name: <myfrontend>
      image: <nginx>
      volumeMounts:
      - mountPath: "/var/www/html"
        name: <mypd>
  volumes:
    - name: <mypd>
      persistentVolumeClaim:
        claimName: <myclaim>


### Simple pod with volume mounts
apiVersion: <v1>
kind: <Pod>
metadata:
  name: <webapp>
spec:
  containers:
  - name: <event-simulator>
    image: <kodekloud/event-simulator>
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: </log>
      name: <log-volume>
      
  volumes:
  - name: <log-volume>
    hostPath:
      # directory location on host
      path: </var/log/webapp>
      # this field is optional
      type: <Directory>


### Storage Class Manifests Flow
apiVersion: <storage.k8s.io/v1>
kind: <StorageClass>
metadata:
   name: <google-storage>
provisioner: <kubernetes.io/gce-pd>
parameters:
  type: <pd-standart / pd-ssd>
  replication-type: <none / regional-pd>

......

kind: <PersistentVolumeClaim>
apiVersion: <v1>
metadata:
  name: <myclaim>
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: <google-storage>   
  resources:
   requests:
     storage: 500Mi
     

### Ingress
apiVersion: <networking.k8s.io/v1>
kind: <Ingress>
metadata:
  name: <minimal-ingress>
  annotations:
    <nginx.ingress.kubernetes.io/rewrite-target: />
spec:
  ingressClassName: <nginx-example>
  rules:
  - http:
      paths:
      - path: </testpath>
        pathType: <Prefix>
        backend:
          service:
            name: <test>
            port:
              number: 80


### Ingress single backend
apiVersion: <networking.k8s.io/v1>
kind: <Ingress>
metadata:
  name: <test-ingress>
spec:
  defaultBackend:
    service:
      name: <test>
      port:
        number: 80

### Nginx controller deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-controller
  namespace: ingress-space
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      serviceAccountName: ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --default-backend-service=app-space/default-http-backend
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443