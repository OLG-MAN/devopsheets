### Command sheets 
### Pre-tips
alias k=kubectl                        
export do="--dry-run=client -o yaml"   

vim ~/.vimrc

set expandtab
set tabstop=2
set shiftwidth=2


1. Contexts
k config view
k config view -o yaml
k config view -o jsonpath="{.contexts[*].name}"
k config get-contexts
k config get-contexts -o name
k config current-context
cat ~/.kube/config | grep current
k config use-context <context-name>

2. Schedule Pod on Master Node
k describe node cluster1-master1 | grep Taint           # get master node taints
k describe node cluster1-master1 | grep Labels -A 10    # get master node labels
k get node cluster1-master1 --show-labels           # OR: get master node labels

3. Scale down StatefulSet
k -n project-c13 scale sts o3db --replicas 1 --record

4. Pod Ready if Service is reachable (see -yamls)

5. Kubectl sorting
k get pod -A --sort-by=.metadata.creationTimestamp
k get pod -A --sort-by=.metadata.uid

6. Storage, PV, PVC, Pod volume (see -yamls)
k -n project-tiger describe pod safari-5cbf46d6d-mjhsb  | grep -A2 Mounts:

7. Node and Pod Resource Usage
kubectl top node
kubectl top pod --containers=true

8. Get Master Information
ps aux | grep kubelet # shows kubelet process
find /etc/systemd/system/ | grep kube    # components are controlled via systemd
find /etc/systemd/system/ | grep etcd
find /etc/kubernetes/manifests/          # components as static-pods

k -n kube-system get pod -o wide | grep <master1> # find dns component
k -n kube-system get ds
k -n kube-system get deploy

9. Kill Scheduler, Manual Scheduling
cd /etc/kubernetes/manifests/
mv kube-scheduler.yaml ..
...pod
nodeName: <master1>
...
cd /etc/kubernetes/manifests/
mv ../kube-scheduler.yaml .

10. RBAC ServiceAccount Role RoleBinding
k -n project-hamster create sa processor
k -n project-hamster create role processor --verb=create --resource=secret,configmap
k -n project-hamster create rolebinding processor --role processor --serviceaccount project-hamster:processor
k auth can-i -h    # examples
k -n project-hamster auth can-i create secret --as system:serviceaccount:project-hamster:processor
<yes>
k -n project-hamster auth can-i get configmap --as system:serviceaccount:project-hamster:processor
<no>

11. DaemonSet on all Nodes (see -yamls)

12. Deployment on all Nodes (see -yamls) 

13. Multi Containers and Pod shared Volume (see -yamls) 

14. Find out Cluster Information
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep range
find /etc/cni/net.d/

15. Cluster Event Logging
k get events -A --sort-by=.metadata.creationTimestamp
crictl ps | grep kube-proxy
crictl rm <container-id>

1.  Namespaces and API Resources
k api-resources -h
k api-resources --namespaced -o name

17. Find Container of Pod and check info
k -n project-tiger run tigers-reunite --image=httpd:2.4.41-alpine --labels "pod=container,container=pod"
crictl ps | grep tigers-reunite
crictl inspect <container-id> | grep runtimeType
ssh <cluster1-worker2> 'crictl logs <container-id>' &> /path/to/log

18. Fix Kubelet
service kubelet status
/etc/systemd/system/kubelet.service.d/10-kubeadm.conf
whereis kubelet
/usr/bin/kubelet
systemctl daemon-reload && systemctl restart kubelet

19. Create Secret and mount into Pod
k -n secret create secret generic secret2 --from-literal=user=user1 --from-literal=pass=1234
k -n secret exec <pod-name> -- env | grep <ENV>
k -n secret exec <pod-name> -- find </path/to/secret>
k -n secret exec <pod-name> -- cat </tmp/secret1/halt>

20. Update Kubernetes Version and join node to cluster
kubeadm version
kubectl version
kubelet --version
kubeadm upgrade node          # errors because node is not joined
apt update
apt show kubectl -a | grep 1.23
apt install kubectl=1.23.1-00 kubelet=1.23.1-00
systemctl restart kubelet
service kubelet status        # errors because node is not joined
### On master Node 
kubeadm token create --print-join-command
<copy kubeadm join command>

### On target node
<apply copied kubeadm join command>
service kubelet status        # status green

1.  Create a Static Pod and Service
cd /etc/kubernetes/manifests/
kubectl run my-static-pod --image=nginx -o yaml --dry-run=client > my-static-pod.yaml
k get svc,ep -l run=my-static-pod

22. Check how long certificates are valid
ls /etc/kubernetes/pki
openssl x509  -noout -text -in /etc/kubernetes/pki/apiserver.crt | grep Validity -A2
kubeadm certs check-expiration | grep apiserver
kubeadm certs renew apiserver

23. Kubelet client/server cert info
ls /var/lib/kubelet/pki
openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep Issuer
openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep "Extended Key Usage" -A1
openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep Issuer
openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep "Extended Key Usage" -A1

24. NetworkPolicy (yaml)
k -n project-snake get pod -o wide
k -n project-snake exec backend-0 -- curl -s 10.44.0.25:1111
<database one>
k -n project-snake exec backend-0 -- curl -s 10.44.0.23:2222
<database two>
k -n project-snake exec backend-0 -- curl -s 10.44.0.22:3333
... <Ctrl+c>

25. Etcd Snapshot Save and Restore (On node)
ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key

### NOTE: Dont use snapshot status because it can alter the snapshot file and render it invalid

cd /etc/kubernetes/manifests/
mv * ..
watch crictl ps

ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \
--data-dir /var/lib/etcd-backup01 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key

<vi /etc/kubernetes/etcd.yaml -> volumes: path: /var/lib/etcd-backup01>

cd /etc/kubernetes/manifests 
mv ../*.yaml .
watch crictl ps

---

### Extra questions:
1. Find Pods first to be terminated
k describe pod | less -p Requests
k describe pod | egrep "^(Name:|    Requests:)" -A1
k get pod \
-o jsonpath="{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\n'}"
k get pods \
-o jsonpath="{range .items[*]}{.metadata.name} {.status.qosClass}{'\n'}"

2. Curl Manually Contact API
k run tmp-api-contact --image=curlimages/curl:7.65.3 $do \
--command > e2.yaml -- sh -c 'sleep 1d'

k -n project-hamster exec tmp-api-contact -it -- sh

curl https://kubernetes.default
curl -k https://kubernetes.default/api/v1/secrets

TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -k https://kubernetes.default/api/v1/secrets -H "Authorization: Bearer ${TOKEN}"

CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
curl --cacert ${CACERT} https://kubernetes.default/api/v1/secrets -H "Authorization: Bearer ${TOKEN}"

### Preview questions:
1. Information about etcd (key, cert expiration), save etcd db, check status.
kubectl -n kube-system get pod
find /etc/kubernetes/manifests/
openssl x509  -noout -text -in /etc/kubernetes/pki/etcd/server.crt | grep Validity -A2
ETCDCTL_API=3 etcdctl snapshot save /etc/etcd-snapshot.db
ETCDCTL_API=3 etcdctl snapshot save /etc/etcd-snapshot.db \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key
ETCDCTL_API=3 etcdctl snapshot status /etc/etcd-snapshot.db

2. check kube-proxy is running correctly on all nodes (yaml)
k -n project-hamster expose pod p2-pod --name p2-service --port 3000 --target-port 80
k -n project-hamster get pod,svc,ep
ssh <master-node>
crictl ps | grep kube-proxy
crictl logs 27b6a18c0f89c    # repeat on all nodes
...
I0913 12:53:03.096620       1 server_others.go:212] Using iptables Proxier.
...
#### Now we check the iptables rules on every node first manually:
ssh <cluster1-master1> iptables-save | grep p2-service

3. CIDR change in cluster (create and expose pod before and after CIDR change)
k run check-ip --image=httpd:2.4.41-alpine
k expose pod check-ip --name check-ip-service --port 80
k get svc,ep -l run=check-ip
vim /etc/kubernetes/manifests/kube-apiserver.yaml -> 
- --service-cluster-ip-range=11.96.0.0/12
vim /etc/kubernetes/manifests/kube-controller-manager.yaml ->
- --service-cluster-ip-range=11.96.0.0/12
crictl ps | grep scheduler
k get pod,svc -l run=check-ip     # check the svc of pod in same network range
k expose pod check-ip --name check-ip-service2 --port 80
k get svc,ep -l run=check-ip      # check new svc of pod create in new network range

