### Application Troubleshooting
#### Pattterns of checking
- Check the object names (pods, services, deployments)
- Check the selectors with right linked object
- Check ports on services
- Check envs inside single pods or deployment 
(e.g. for mysql - users, pass, hosts)

1. Troubleshooting Test 1: A simple 2 tier application is deployed in the alpha namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
1. Solution. Check webapp deployment and DB_HOST value that linked to mysql service. Name of mysql svc is incorrect (mysql -> mysql-service). Recreate mysql service with right name.


2. Troubleshooting Test 2: The same 2 tier application is deployed in the beta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
2. Solution. Check the ports of svc mysql-service, change target port to right 8080->3306. Edit changes.


3. The same 2 tier application is deployed in the gamma namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
3. Solution. Check the mysql service and selectors in it. Change selector to right app name sql0001->mysql. Edit changes.


4. Troubleshooting Test 4: The same 2 tier application is deployed in the delta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
4. Solution. Check env values in deployment webapp-mysql, change DB_USER to right name sql-user->root. Edit changes.


5. The same 2 tier application is deployed in the epsilon namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
5.  Solution. Check the env in mysql pod change password to right (paswrd). Check username in deployment webapp-mysql change to right value -> root.


6.  Troubleshooting Test 6: The same 2 tier application is deployed in the zeta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
6. Solution. Check webapp-service and edit Nodeport to right 30088 -> 30081. Next check the deployment webapp-mysql and change DB_USER to -> root. 
Last check mysql pod and edit env value of password -> paswrd. 
You can't edit mysql password env in real-time.
You need get data of pod to yaml, edit it, next delete the pod and recrete new one. 


### Cluster Controlplane Troubleshooting
#### Pattterns of checking.
- Check kube-system objects
- Check kube-system objects manifests
- Check all commands, paths to configuration files, folder paths etc.


1. The cluster is broken. We tried deploying an application but it's not working. Troubleshoot and fix the issue.
Start looking at the deployments.
1. Solution. Check k8s components in kube-system namespace
kubectl get pods -n kube-system
edit scheduler yaml file, check commnds and edit to right value.
vi /etc/kubernetes/manifests/kube-scheduler.yaml


2. Scale the deployment app to 2 pods.
2. Solution. Scale replicas in target deployment.
kubectl scale deploy app --replicas=2


3. Even though the deployment was scaled to 2, the number of PODs does not seem to increase. Investigate and fix the issue.
Inspect the component responsible for managing deployments and replicasets.
3. Solution. Check kube-system pods. Noticed problem with cm. 
kubectl get po -n kube-system
Edit controller-manager manifest file, change .conf file to right value
vi /etc/kubernetes/manifests/kube-controller-manager.yaml


4. Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. But it's not happening.
4. Solution. Check kube-system objects. cm have an issue. edit manifest and chnage pki folder path to right -> /etc/kubernetes/pki, save changes.
kubectl get po -n kube-system
vi /etc/kubernetes/manifests/kube-controller-manager.yaml



### Cluster worker nodes Troubleshooting
#### Pattterns of checking.
- Check kubelet service status
- Check issue with values in file /var/lib/kubelet/config.yaml
- Check issue with values in file /etc/kubernetes/kubelet.conf


1. Fix the broken cluster
Go to node01 and restart kubelet service.
```
kubectl get nodes
ssh node01
service kubelet status
service kubelet start
service kubelet status
exit
kubectl get nodes
```

2. The cluster is broken again. Investigate and fix the issue.
Go to node01 check kubelet again and fix it.
Issue in /var/lib/kubelet/config.yaml fix ca.crt path:
```
ssh node01 
journalctl -u kubelet
edit path to ca.crt and save it.
vi /var/lib/kubelet/config.yaml
service kubelet restart
service kubelet status
exit
kubectl get nodes
```

3. The cluster is broken again. Investigate and fix the issue.
Go to node01 check kubelet again and fix it. 
Issue in /etc/kubernetes/kubelet.conf  fix cluster port:
```
ssh node01 
journalctl -u kubelet
edit path to ca.crt and save it.
vi /var/lib/kubelet/config.yaml
service kubelet restart
service kubelet status
exit
kubectl get nodes
```


### Network Troubleshooting
#### Pattterns of checking.
- Check CNI existing in cluster
- Check kube-proxy ds, conf filed paths etc.
- Check cm's in kube-system ns 


1. A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application.
2. Solution. Check that kube-proxy is exists in cluster (CNI).
Install weave CNI (for example) to cluster. 
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


2. The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue. 
2. Check logs in kube-proxy, change path to config file in ds.




### Network Troubleshooting Theory
#### Network Plugin in kubernetes
Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration.
– cni-bin-dir:  Kubelet probes this directory for plugins on startup
– network-plugin: The network plugin to use from cni-bin-dir. It must match the name reported by a plugin probed from the plugin directory.
There are several plugins available and these are some.
#### links
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
1. Weave Net:
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
2. Flannel :
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
#### Note: As of now flannel does not support kubernetes network policies.
3. Calico :
```
curl https://docs.projectcalico.org/manifests/calico.yaml -O
kubectl apply -f calico.yaml
```
```
Calico is said to have most advanced cni network plugin.
In CKA and CKAD exam, you won’t be asked to install the cni plugin. But if asked you will be provided with the exact url to install it. If not, you can install weave net from the documentation
```

##### Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.

#### DNS in Kubernetes
Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.

#### Memory and Pods
In large scale Kubernetes clusters, CoreDNS’s memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance.

#### Kubernetes resources for coreDNS are:
```
a Service account      coredns,
cluster-roles          coredns and kube-dns
clusterrolebindings    coredns and kube-dns, 
a deployment           coredns,
a configmap            coredns 
a service              kube-dns.
```

##### While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.

Port 53 is used for for DNS resolution.
```
kubernetes cluster.local in-addr.arpa ip6.arpa {
    pods insecure
    fallthrough in-addr.arpa ip6.arpa
    ttl 30
}
```
This is the backend to k8s for cluster.local and reverse domains.

proxy . /etc/resolv.conf


#### Troubleshooting issues related to coreDNS
1. If you find CoreDNS pods in pending state first check network plugin is installed.

2. coredns pods have CrashLoopBackOff or Error state
If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options:

a)Upgrade to a newer version of Docker.

b)Disable SELinux

c)Modify the coredns deployment to set allowPrivilegeEscalation to true:
```
kubectl -n kube-system get deployment coredns -o yaml | \
sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
kubectl apply -f -
```

d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.

There are many ways to work around this issue, some are listed here:
Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. 
For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the “real” resolv.conf, although this can be different depending on your distribution.
Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.
A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS, for example forward . 8.8.8.8. But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.


3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.
```
kubectl -n kube-system get ep kube-dns
```
If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.



### Troubleshooting issues related to Kube-Proxy
kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.

In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.

kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.

If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container.

    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
 

So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

Troubleshooting issues related to kube-proxy:

1. Check kube-proxy pod in the kube-system namespace is running.
2. Check kube-proxy logs.
3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.
4. kube-config is defined in the config map.
5. check kube-proxy is running inside the container
```
# netstat -plan | grep kube-proxy
tcp        0      0 0.0.0.0:30081           0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 172.17.0.12:33706       172.17.0.12:6443        ESTABLISHED 1/kube-proxy
tcp6       0      0 :::10256                :::*                    LISTEN      1/kube-proxy
References:
```

Debug Service issues:
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/

DNS Troubleshooting:
https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/

